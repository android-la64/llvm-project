; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc --mtriple=loongarch64 < %s | FileCheck %s --check-prefix=LA64

define void @cmpxchg_i8_acquire_acquire(i8* %ptr, i8 %cmp, i8 %val) nounwind {
; LA64-LABEL: cmpxchg_i8_acquire_acquire:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r4, $r4, 3
; LA64-NEXT:    ori $r8, $zero, 255
; LA64-NEXT:    sll.w $r8, $r8, $r4
; LA64-NEXT:    nor $r9, $zero, $r8
; LA64-NEXT:    andi $r5, $r5, 255
; LA64-NEXT:    sll.w $r5, $r5, $r4
; LA64-NEXT:    andi $r6, $r6, 255
; LA64-NEXT:    sll.w $r6, $r6, $r4
; LA64-NEXT:  .LBB0_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r11, $r7, 0
; LA64-NEXT:    and $r12, $r11, $r8
; LA64-NEXT:    bne $r12, $r5, .LBB0_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB0_1 Depth=1
; LA64-NEXT:    and $r11, $r11, $r9
; LA64-NEXT:    or $r11, $r11, $r6
; LA64-NEXT:    sc.w $r11, $r7, 0
; LA64-NEXT:    beq $r11, $zero, .LBB0_1
; LA64-NEXT:  .LBB0_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    srl.w $r10, $r12, $r4
; LA64-NEXT:    ext.w.b $r10, $r10
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    jr $ra
  %res = cmpxchg i8* %ptr, i8 %cmp, i8 %val acquire acquire
  ret void
}

define void @cmpxchg_i8_release_acquire(i8* %ptr, i8 %cmp, i8 %val) nounwind {
; LA64-LABEL: cmpxchg_i8_release_acquire:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r4, $r4, 3
; LA64-NEXT:    ori $r8, $zero, 255
; LA64-NEXT:    sll.w $r8, $r8, $r4
; LA64-NEXT:    nor $r9, $zero, $r8
; LA64-NEXT:    andi $r5, $r5, 255
; LA64-NEXT:    sll.w $r5, $r5, $r4
; LA64-NEXT:    andi $r6, $r6, 255
; LA64-NEXT:    sll.w $r6, $r6, $r4
; LA64-NEXT:  .LBB1_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r11, $r7, 0
; LA64-NEXT:    and $r12, $r11, $r8
; LA64-NEXT:    bne $r12, $r5, .LBB1_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB1_1 Depth=1
; LA64-NEXT:    and $r11, $r11, $r9
; LA64-NEXT:    or $r11, $r11, $r6
; LA64-NEXT:    sc.w $r11, $r7, 0
; LA64-NEXT:    beq $r11, $zero, .LBB1_1
; LA64-NEXT:  .LBB1_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    srl.w $r10, $r12, $r4
; LA64-NEXT:    ext.w.b $r10, $r10
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    jr $ra
  %res = cmpxchg i8* %ptr, i8 %cmp, i8 %val release acquire
  ret void
}

;; Check that only the failure ordering is taken care.
define void @cmpxchg_i8_acquire_monotonic(i8* %ptr, i8 %cmp, i8 %val) nounwind {
; LA64-LABEL: cmpxchg_i8_acquire_monotonic:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r4, $r4, 3
; LA64-NEXT:    ori $r8, $zero, 255
; LA64-NEXT:    sll.w $r8, $r8, $r4
; LA64-NEXT:    nor $r9, $zero, $r8
; LA64-NEXT:    andi $r5, $r5, 255
; LA64-NEXT:    sll.w $r5, $r5, $r4
; LA64-NEXT:    andi $r6, $r6, 255
; LA64-NEXT:    sll.w $r6, $r6, $r4
; LA64-NEXT:  .LBB2_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r11, $r7, 0
; LA64-NEXT:    and $r12, $r11, $r8
; LA64-NEXT:    bne $r12, $r5, .LBB2_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB2_1 Depth=1
; LA64-NEXT:    and $r11, $r11, $r9
; LA64-NEXT:    or $r11, $r11, $r6
; LA64-NEXT:    sc.w $r11, $r7, 0
; LA64-NEXT:    beq $r11, $zero, .LBB2_1
; LA64-NEXT:  .LBB2_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    srl.w $r10, $r12, $r4
; LA64-NEXT:    ext.w.b $r10, $r10
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    jr $ra
  %res = cmpxchg i8* %ptr, i8 %cmp, i8 %val acquire monotonic
  ret void
}

define void @cmpxchg_i16_acquire_acquire(i16* %ptr, i16 %cmp, i16 %val) nounwind {
; LA64-LABEL: cmpxchg_i16_acquire_acquire:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r4, $r4, 3
; LA64-NEXT:    lu12i.w $r8, 15
; LA64-NEXT:    ori $r8, $r8, 4095
; LA64-NEXT:    sll.w $r9, $r8, $r4
; LA64-NEXT:    nor $r10, $zero, $r9
; LA64-NEXT:    and $r5, $r5, $r8
; LA64-NEXT:    sll.w $r5, $r5, $r4
; LA64-NEXT:    and $r6, $r6, $r8
; LA64-NEXT:    sll.w $r6, $r6, $r4
; LA64-NEXT:  .LBB3_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r11, $r7, 0
; LA64-NEXT:    and $r12, $r11, $r9
; LA64-NEXT:    bne $r12, $r5, .LBB3_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB3_1 Depth=1
; LA64-NEXT:    and $r11, $r11, $r10
; LA64-NEXT:    or $r11, $r11, $r6
; LA64-NEXT:    sc.w $r11, $r7, 0
; LA64-NEXT:    beq $r11, $zero, .LBB3_1
; LA64-NEXT:  .LBB3_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    srl.w $r8, $r12, $r4
; LA64-NEXT:    ext.w.h $r8, $r8
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    jr $ra
  %res = cmpxchg i16* %ptr, i16 %cmp, i16 %val acquire acquire
  ret void
}

define void @cmpxchg_i16_release_acquire(i16* %ptr, i16 %cmp, i16 %val) nounwind {
; LA64-LABEL: cmpxchg_i16_release_acquire:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r4, $r4, 3
; LA64-NEXT:    lu12i.w $r8, 15
; LA64-NEXT:    ori $r8, $r8, 4095
; LA64-NEXT:    sll.w $r9, $r8, $r4
; LA64-NEXT:    nor $r10, $zero, $r9
; LA64-NEXT:    and $r5, $r5, $r8
; LA64-NEXT:    sll.w $r5, $r5, $r4
; LA64-NEXT:    and $r6, $r6, $r8
; LA64-NEXT:    sll.w $r6, $r6, $r4
; LA64-NEXT:  .LBB4_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r11, $r7, 0
; LA64-NEXT:    and $r12, $r11, $r9
; LA64-NEXT:    bne $r12, $r5, .LBB4_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB4_1 Depth=1
; LA64-NEXT:    and $r11, $r11, $r10
; LA64-NEXT:    or $r11, $r11, $r6
; LA64-NEXT:    sc.w $r11, $r7, 0
; LA64-NEXT:    beq $r11, $zero, .LBB4_1
; LA64-NEXT:  .LBB4_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    srl.w $r8, $r12, $r4
; LA64-NEXT:    ext.w.h $r8, $r8
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    jr $ra
  %res = cmpxchg i16* %ptr, i16 %cmp, i16 %val release acquire
  ret void
}

;; Check that only the failure ordering is taken care.
define void @cmpxchg_i16_acquire_monotonic(i16* %ptr, i16 %cmp, i16 %val) nounwind {
; LA64-LABEL: cmpxchg_i16_acquire_monotonic:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r4, $r4, 3
; LA64-NEXT:    lu12i.w $r8, 15
; LA64-NEXT:    ori $r8, $r8, 4095
; LA64-NEXT:    sll.w $r9, $r8, $r4
; LA64-NEXT:    nor $r10, $zero, $r9
; LA64-NEXT:    and $r5, $r5, $r8
; LA64-NEXT:    sll.w $r5, $r5, $r4
; LA64-NEXT:    and $r6, $r6, $r8
; LA64-NEXT:    sll.w $r6, $r6, $r4
; LA64-NEXT:  .LBB5_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r11, $r7, 0
; LA64-NEXT:    and $r12, $r11, $r9
; LA64-NEXT:    bne $r12, $r5, .LBB5_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB5_1 Depth=1
; LA64-NEXT:    and $r11, $r11, $r10
; LA64-NEXT:    or $r11, $r11, $r6
; LA64-NEXT:    sc.w $r11, $r7, 0
; LA64-NEXT:    beq $r11, $zero, .LBB5_1
; LA64-NEXT:  .LBB5_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    srl.w $r8, $r12, $r4
; LA64-NEXT:    ext.w.h $r8, $r8
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    jr $ra
  %res = cmpxchg i16* %ptr, i16 %cmp, i16 %val acquire monotonic
  ret void
}

define void @cmpxchg_i32_acquire_acquire(i32* %ptr, i32 %cmp, i32 %val) nounwind {
; LA64-LABEL: cmpxchg_i32_acquire_acquire:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:  .LBB6_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r7, $r4, 0
; LA64-NEXT:    bne $r7, $r5, .LBB6_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB6_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.w $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB6_1
; LA64-NEXT:  .LBB6_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    jr $ra
  %res = cmpxchg i32* %ptr, i32 %cmp, i32 %val acquire acquire
  ret void
}

define void @cmpxchg_i32_release_acquire(i32* %ptr, i32 %cmp, i32 %val) nounwind {
; LA64-LABEL: cmpxchg_i32_release_acquire:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:  .LBB7_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r7, $r4, 0
; LA64-NEXT:    bne $r7, $r5, .LBB7_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB7_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.w $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB7_1
; LA64-NEXT:  .LBB7_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    jr $ra
  %res = cmpxchg i32* %ptr, i32 %cmp, i32 %val release acquire
  ret void
}

;; Check that only the failure ordering is taken care.
define void @cmpxchg_i32_acquire_monotonic(i32* %ptr, i32 %cmp, i32 %val) nounwind {
; LA64-LABEL: cmpxchg_i32_acquire_monotonic:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:  .LBB8_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r7, $r4, 0
; LA64-NEXT:    bne $r7, $r5, .LBB8_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB8_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.w $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB8_1
; LA64-NEXT:  .LBB8_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    jr $ra
  %res = cmpxchg i32* %ptr, i32 %cmp, i32 %val acquire monotonic
  ret void
}

define void @cmpxchg_i64_acquire_acquire(i64* %ptr, i64 %cmp, i64 %val) nounwind {
; LA64-LABEL: cmpxchg_i64_acquire_acquire:
; LA64:       # %bb.0:
; LA64-NEXT:  .LBB9_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.d $r7, $r4, 0
; LA64-NEXT:    bne $r7, $r5, .LBB9_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB9_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.d $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB9_1
; LA64-NEXT:  .LBB9_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    jr $ra
  %res = cmpxchg i64* %ptr, i64 %cmp, i64 %val acquire acquire
  ret void
}

define void @cmpxchg_i64_release_acquire(i64* %ptr, i64 %cmp, i64 %val) nounwind {
; LA64-LABEL: cmpxchg_i64_release_acquire:
; LA64:       # %bb.0:
; LA64-NEXT:  .LBB10_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.d $r7, $r4, 0
; LA64-NEXT:    bne $r7, $r5, .LBB10_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB10_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.d $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB10_1
; LA64-NEXT:  .LBB10_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    jr $ra
  %res = cmpxchg i64* %ptr, i64 %cmp, i64 %val release acquire
  ret void
}

;; Check that only the failure ordering is taken care.
define void @cmpxchg_i64_acquire_monotonic(i64* %ptr, i64 %cmp, i64 %val) nounwind {
; LA64-LABEL: cmpxchg_i64_acquire_monotonic:
; LA64:       # %bb.0:
; LA64-NEXT:  .LBB11_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.d $r7, $r4, 0
; LA64-NEXT:    bne $r7, $r5, .LBB11_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB11_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.d $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB11_1
; LA64-NEXT:  .LBB11_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    jr $ra
  %res = cmpxchg i64* %ptr, i64 %cmp, i64 %val acquire monotonic
  ret void
}

define i8 @cmpxchg_i8_acquire_acquire_reti8(i8* %ptr, i8 %cmp, i8 %val) nounwind {
; LA64-LABEL: cmpxchg_i8_acquire_acquire_reti8:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r8, $r4, 3
; LA64-NEXT:    ori $r4, $zero, 255
; LA64-NEXT:    sll.w $r9, $r4, $r8
; LA64-NEXT:    nor $r10, $zero, $r9
; LA64-NEXT:    andi $r4, $r5, 255
; LA64-NEXT:    sll.w $r5, $r4, $r8
; LA64-NEXT:    andi $r4, $r6, 255
; LA64-NEXT:    sll.w $r6, $r4, $r8
; LA64-NEXT:  .LBB12_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r11, $r7, 0
; LA64-NEXT:    and $r12, $r11, $r9
; LA64-NEXT:    bne $r12, $r5, .LBB12_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB12_1 Depth=1
; LA64-NEXT:    and $r11, $r11, $r10
; LA64-NEXT:    or $r11, $r11, $r6
; LA64-NEXT:    sc.w $r11, $r7, 0
; LA64-NEXT:    beq $r11, $zero, .LBB12_1
; LA64-NEXT:  .LBB12_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    srl.w $r4, $r12, $r8
; LA64-NEXT:    ext.w.b $r4, $r4
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i8* %ptr, i8 %cmp, i8 %val acquire acquire
  %res = extractvalue { i8, i1 } %tmp, 0
  ret i8 %res
}

define i16 @cmpxchg_i16_acquire_acquire_reti16(i16* %ptr, i16 %cmp, i16 %val) nounwind {
; LA64-LABEL: cmpxchg_i16_acquire_acquire_reti16:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r8, $r4, 3
; LA64-NEXT:    lu12i.w $r4, 15
; LA64-NEXT:    ori $r4, $r4, 4095
; LA64-NEXT:    sll.w $r9, $r4, $r8
; LA64-NEXT:    nor $r10, $zero, $r9
; LA64-NEXT:    and $r5, $r5, $r4
; LA64-NEXT:    sll.w $r5, $r5, $r8
; LA64-NEXT:    and $r4, $r6, $r4
; LA64-NEXT:    sll.w $r6, $r4, $r8
; LA64-NEXT:  .LBB13_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r11, $r7, 0
; LA64-NEXT:    and $r12, $r11, $r9
; LA64-NEXT:    bne $r12, $r5, .LBB13_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB13_1 Depth=1
; LA64-NEXT:    and $r11, $r11, $r10
; LA64-NEXT:    or $r11, $r11, $r6
; LA64-NEXT:    sc.w $r11, $r7, 0
; LA64-NEXT:    beq $r11, $zero, .LBB13_1
; LA64-NEXT:  .LBB13_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    srl.w $r4, $r12, $r8
; LA64-NEXT:    ext.w.h $r4, $r4
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i16* %ptr, i16 %cmp, i16 %val acquire acquire
  %res = extractvalue { i16, i1 } %tmp, 0
  ret i16 %res
}

define i32 @cmpxchg_i32_acquire_acquire_reti32(i32* %ptr, i32 %cmp, i32 %val) nounwind {
; LA64-LABEL: cmpxchg_i32_acquire_acquire_reti32:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r7, $r5, 0
; LA64-NEXT:  .LBB14_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r5, $r4, 0
; LA64-NEXT:    bne $r5, $r7, .LBB14_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB14_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.w $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB14_1
; LA64-NEXT:  .LBB14_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    move $r4, $r5
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i32* %ptr, i32 %cmp, i32 %val acquire acquire
  %res = extractvalue { i32, i1 } %tmp, 0
  ret i32 %res
}

define i64 @cmpxchg_i64_acquire_acquire_reti64(i64* %ptr, i64 %cmp, i64 %val) nounwind {
; LA64-LABEL: cmpxchg_i64_acquire_acquire_reti64:
; LA64:       # %bb.0:
; LA64-NEXT:  .LBB15_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.d $r7, $r4, 0
; LA64-NEXT:    bne $r7, $r5, .LBB15_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB15_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.d $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB15_1
; LA64-NEXT:  .LBB15_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    move $r4, $r7
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i64* %ptr, i64 %cmp, i64 %val acquire acquire
  %res = extractvalue { i64, i1 } %tmp, 0
  ret i64 %res
}

define i1 @cmpxchg_i8_acquire_acquire_reti1(i8* %ptr, i8 %cmp, i8 %val) nounwind {
; LA64-LABEL: cmpxchg_i8_acquire_acquire_reti1:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r8, $r4, 3
; LA64-NEXT:    ori $r4, $zero, 255
; LA64-NEXT:    sll.w $r9, $r4, $r8
; LA64-NEXT:    nor $r10, $zero, $r9
; LA64-NEXT:    andi $r4, $r5, 255
; LA64-NEXT:    sll.w $r11, $r4, $r8
; LA64-NEXT:    andi $r4, $r6, 255
; LA64-NEXT:    sll.w $r6, $r4, $r8
; LA64-NEXT:  .LBB16_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r12, $r7, 0
; LA64-NEXT:    and $r13, $r12, $r9
; LA64-NEXT:    bne $r13, $r11, .LBB16_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB16_1 Depth=1
; LA64-NEXT:    and $r12, $r12, $r10
; LA64-NEXT:    or $r12, $r12, $r6
; LA64-NEXT:    sc.w $r12, $r7, 0
; LA64-NEXT:    beq $r12, $zero, .LBB16_1
; LA64-NEXT:  .LBB16_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    srl.w $r4, $r13, $r8
; LA64-NEXT:    ext.w.b $r4, $r4
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    ext.w.b $r5, $r5
; LA64-NEXT:    xor $r4, $r4, $r5
; LA64-NEXT:    sltui $r4, $r4, 1
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i8* %ptr, i8 %cmp, i8 %val acquire acquire
  %res = extractvalue { i8, i1 } %tmp, 1
  ret i1 %res
}

define i1 @cmpxchg_i16_acquire_acquire_reti1(i16* %ptr, i16 %cmp, i16 %val) nounwind {
; LA64-LABEL: cmpxchg_i16_acquire_acquire_reti1:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r8, $r4, 3
; LA64-NEXT:    lu12i.w $r4, 15
; LA64-NEXT:    ori $r4, $r4, 4095
; LA64-NEXT:    sll.w $r9, $r4, $r8
; LA64-NEXT:    nor $r10, $zero, $r9
; LA64-NEXT:    and $r11, $r5, $r4
; LA64-NEXT:    sll.w $r11, $r11, $r8
; LA64-NEXT:    and $r4, $r6, $r4
; LA64-NEXT:    sll.w $r6, $r4, $r8
; LA64-NEXT:  .LBB17_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r12, $r7, 0
; LA64-NEXT:    and $r13, $r12, $r9
; LA64-NEXT:    bne $r13, $r11, .LBB17_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB17_1 Depth=1
; LA64-NEXT:    and $r12, $r12, $r10
; LA64-NEXT:    or $r12, $r12, $r6
; LA64-NEXT:    sc.w $r12, $r7, 0
; LA64-NEXT:    beq $r12, $zero, .LBB17_1
; LA64-NEXT:  .LBB17_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    srl.w $r4, $r13, $r8
; LA64-NEXT:    ext.w.h $r4, $r4
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    ext.w.h $r5, $r5
; LA64-NEXT:    xor $r4, $r4, $r5
; LA64-NEXT:    sltui $r4, $r4, 1
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i16* %ptr, i16 %cmp, i16 %val acquire acquire
  %res = extractvalue { i16, i1 } %tmp, 1
  ret i1 %res
}

define i1 @cmpxchg_i32_acquire_acquire_reti1(i32* %ptr, i32 %cmp, i32 %val) nounwind {
; LA64-LABEL: cmpxchg_i32_acquire_acquire_reti1:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:  .LBB18_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r7, $r4, 0
; LA64-NEXT:    bne $r7, $r5, .LBB18_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB18_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.w $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB18_1
; LA64-NEXT:  .LBB18_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    xor $r4, $r7, $r5
; LA64-NEXT:    sltui $r4, $r4, 1
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i32* %ptr, i32 %cmp, i32 %val acquire acquire
  %res = extractvalue { i32, i1 } %tmp, 1
  ret i1 %res
}

define i1 @cmpxchg_i64_acquire_acquire_reti1(i64* %ptr, i64 %cmp, i64 %val) nounwind {
; LA64-LABEL: cmpxchg_i64_acquire_acquire_reti1:
; LA64:       # %bb.0:
; LA64-NEXT:  .LBB19_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.d $r7, $r4, 0
; LA64-NEXT:    bne $r7, $r5, .LBB19_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB19_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.d $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB19_1
; LA64-NEXT:  .LBB19_3:
; LA64-NEXT:    dbar 0
; LA64-NEXT:    xor $r4, $r7, $r5
; LA64-NEXT:    sltui $r4, $r4, 1
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i64* %ptr, i64 %cmp, i64 %val acquire acquire
  %res = extractvalue { i64, i1 } %tmp, 1
  ret i1 %res
}

define void @cmpxchg_i8_monotonic_monotonic(i8* %ptr, i8 %cmp, i8 %val) nounwind {
; LA64-LABEL: cmpxchg_i8_monotonic_monotonic:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r4, $r4, 3
; LA64-NEXT:    ori $r8, $zero, 255
; LA64-NEXT:    sll.w $r8, $r8, $r4
; LA64-NEXT:    nor $r9, $zero, $r8
; LA64-NEXT:    andi $r5, $r5, 255
; LA64-NEXT:    sll.w $r5, $r5, $r4
; LA64-NEXT:    andi $r6, $r6, 255
; LA64-NEXT:    sll.w $r6, $r6, $r4
; LA64-NEXT:  .LBB20_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r11, $r7, 0
; LA64-NEXT:    and $r12, $r11, $r8
; LA64-NEXT:    bne $r12, $r5, .LBB20_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB20_1 Depth=1
; LA64-NEXT:    and $r11, $r11, $r9
; LA64-NEXT:    or $r11, $r11, $r6
; LA64-NEXT:    sc.w $r11, $r7, 0
; LA64-NEXT:    beq $r11, $zero, .LBB20_1
; LA64-NEXT:  .LBB20_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    srl.w $r10, $r12, $r4
; LA64-NEXT:    ext.w.b $r10, $r10
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    jr $ra
  %res = cmpxchg i8* %ptr, i8 %cmp, i8 %val monotonic monotonic
  ret void
}

define void @cmpxchg_i16_monotonic_monotonic(i16* %ptr, i16 %cmp, i16 %val) nounwind {
; LA64-LABEL: cmpxchg_i16_monotonic_monotonic:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r4, $r4, 3
; LA64-NEXT:    lu12i.w $r8, 15
; LA64-NEXT:    ori $r8, $r8, 4095
; LA64-NEXT:    sll.w $r9, $r8, $r4
; LA64-NEXT:    nor $r10, $zero, $r9
; LA64-NEXT:    and $r5, $r5, $r8
; LA64-NEXT:    sll.w $r5, $r5, $r4
; LA64-NEXT:    and $r6, $r6, $r8
; LA64-NEXT:    sll.w $r6, $r6, $r4
; LA64-NEXT:  .LBB21_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r11, $r7, 0
; LA64-NEXT:    and $r12, $r11, $r9
; LA64-NEXT:    bne $r12, $r5, .LBB21_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB21_1 Depth=1
; LA64-NEXT:    and $r11, $r11, $r10
; LA64-NEXT:    or $r11, $r11, $r6
; LA64-NEXT:    sc.w $r11, $r7, 0
; LA64-NEXT:    beq $r11, $zero, .LBB21_1
; LA64-NEXT:  .LBB21_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    srl.w $r8, $r12, $r4
; LA64-NEXT:    ext.w.h $r8, $r8
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    jr $ra
  %res = cmpxchg i16* %ptr, i16 %cmp, i16 %val monotonic monotonic
  ret void
}

define void @cmpxchg_i32_monotonic_monotonic(i32* %ptr, i32 %cmp, i32 %val) nounwind {
; LA64-LABEL: cmpxchg_i32_monotonic_monotonic:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:  .LBB22_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r7, $r4, 0
; LA64-NEXT:    bne $r7, $r5, .LBB22_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB22_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.w $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB22_1
; LA64-NEXT:  .LBB22_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    jr $ra
  %res = cmpxchg i32* %ptr, i32 %cmp, i32 %val monotonic monotonic
  ret void
}

define void @cmpxchg_i64_monotonic_monotonic(i64* %ptr, i64 %cmp, i64 %val) nounwind {
; LA64-LABEL: cmpxchg_i64_monotonic_monotonic:
; LA64:       # %bb.0:
; LA64-NEXT:  .LBB23_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.d $r7, $r4, 0
; LA64-NEXT:    bne $r7, $r5, .LBB23_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB23_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.d $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB23_1
; LA64-NEXT:  .LBB23_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    jr $ra
  %res = cmpxchg i64* %ptr, i64 %cmp, i64 %val monotonic monotonic
  ret void
}

define i8 @cmpxchg_i8_monotonic_monotonic_reti8(i8* %ptr, i8 %cmp, i8 %val) nounwind {
; LA64-LABEL: cmpxchg_i8_monotonic_monotonic_reti8:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r8, $r4, 3
; LA64-NEXT:    ori $r4, $zero, 255
; LA64-NEXT:    sll.w $r9, $r4, $r8
; LA64-NEXT:    nor $r10, $zero, $r9
; LA64-NEXT:    andi $r4, $r5, 255
; LA64-NEXT:    sll.w $r5, $r4, $r8
; LA64-NEXT:    andi $r4, $r6, 255
; LA64-NEXT:    sll.w $r6, $r4, $r8
; LA64-NEXT:  .LBB24_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r11, $r7, 0
; LA64-NEXT:    and $r12, $r11, $r9
; LA64-NEXT:    bne $r12, $r5, .LBB24_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB24_1 Depth=1
; LA64-NEXT:    and $r11, $r11, $r10
; LA64-NEXT:    or $r11, $r11, $r6
; LA64-NEXT:    sc.w $r11, $r7, 0
; LA64-NEXT:    beq $r11, $zero, .LBB24_1
; LA64-NEXT:  .LBB24_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    srl.w $r4, $r12, $r8
; LA64-NEXT:    ext.w.b $r4, $r4
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i8* %ptr, i8 %cmp, i8 %val monotonic monotonic
  %res = extractvalue { i8, i1 } %tmp, 0
  ret i8 %res
}

define i16 @cmpxchg_i16_monotonic_monotonic_reti16(i16* %ptr, i16 %cmp, i16 %val) nounwind {
; LA64-LABEL: cmpxchg_i16_monotonic_monotonic_reti16:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r8, $r4, 3
; LA64-NEXT:    lu12i.w $r4, 15
; LA64-NEXT:    ori $r4, $r4, 4095
; LA64-NEXT:    sll.w $r9, $r4, $r8
; LA64-NEXT:    nor $r10, $zero, $r9
; LA64-NEXT:    and $r5, $r5, $r4
; LA64-NEXT:    sll.w $r5, $r5, $r8
; LA64-NEXT:    and $r4, $r6, $r4
; LA64-NEXT:    sll.w $r6, $r4, $r8
; LA64-NEXT:  .LBB25_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r11, $r7, 0
; LA64-NEXT:    and $r12, $r11, $r9
; LA64-NEXT:    bne $r12, $r5, .LBB25_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB25_1 Depth=1
; LA64-NEXT:    and $r11, $r11, $r10
; LA64-NEXT:    or $r11, $r11, $r6
; LA64-NEXT:    sc.w $r11, $r7, 0
; LA64-NEXT:    beq $r11, $zero, .LBB25_1
; LA64-NEXT:  .LBB25_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    srl.w $r4, $r12, $r8
; LA64-NEXT:    ext.w.h $r4, $r4
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i16* %ptr, i16 %cmp, i16 %val monotonic monotonic
  %res = extractvalue { i16, i1 } %tmp, 0
  ret i16 %res
}

define i32 @cmpxchg_i32_monotonic_monotonic_reti32(i32* %ptr, i32 %cmp, i32 %val) nounwind {
; LA64-LABEL: cmpxchg_i32_monotonic_monotonic_reti32:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r7, $r5, 0
; LA64-NEXT:  .LBB26_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r5, $r4, 0
; LA64-NEXT:    bne $r5, $r7, .LBB26_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB26_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.w $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB26_1
; LA64-NEXT:  .LBB26_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    move $r4, $r5
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i32* %ptr, i32 %cmp, i32 %val monotonic monotonic
  %res = extractvalue { i32, i1 } %tmp, 0
  ret i32 %res
}

define i64 @cmpxchg_i64_monotonic_monotonic_reti64(i64* %ptr, i64 %cmp, i64 %val) nounwind {
; LA64-LABEL: cmpxchg_i64_monotonic_monotonic_reti64:
; LA64:       # %bb.0:
; LA64-NEXT:  .LBB27_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.d $r7, $r4, 0
; LA64-NEXT:    bne $r7, $r5, .LBB27_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB27_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.d $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB27_1
; LA64-NEXT:  .LBB27_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    move $r4, $r7
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i64* %ptr, i64 %cmp, i64 %val monotonic monotonic
  %res = extractvalue { i64, i1 } %tmp, 0
  ret i64 %res
}

define i1 @cmpxchg_i8_monotonic_monotonic_reti1(i8* %ptr, i8 %cmp, i8 %val) nounwind {
; LA64-LABEL: cmpxchg_i8_monotonic_monotonic_reti1:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r8, $r4, 3
; LA64-NEXT:    ori $r4, $zero, 255
; LA64-NEXT:    sll.w $r9, $r4, $r8
; LA64-NEXT:    nor $r10, $zero, $r9
; LA64-NEXT:    andi $r4, $r5, 255
; LA64-NEXT:    sll.w $r11, $r4, $r8
; LA64-NEXT:    andi $r4, $r6, 255
; LA64-NEXT:    sll.w $r6, $r4, $r8
; LA64-NEXT:  .LBB28_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r12, $r7, 0
; LA64-NEXT:    and $r13, $r12, $r9
; LA64-NEXT:    bne $r13, $r11, .LBB28_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB28_1 Depth=1
; LA64-NEXT:    and $r12, $r12, $r10
; LA64-NEXT:    or $r12, $r12, $r6
; LA64-NEXT:    sc.w $r12, $r7, 0
; LA64-NEXT:    beq $r12, $zero, .LBB28_1
; LA64-NEXT:  .LBB28_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    srl.w $r4, $r13, $r8
; LA64-NEXT:    ext.w.b $r4, $r4
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    ext.w.b $r5, $r5
; LA64-NEXT:    xor $r4, $r4, $r5
; LA64-NEXT:    sltui $r4, $r4, 1
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i8* %ptr, i8 %cmp, i8 %val monotonic monotonic
  %res = extractvalue { i8, i1 } %tmp, 1
  ret i1 %res
}

define i1 @cmpxchg_i16_monotonic_monotonic_reti1(i16* %ptr, i16 %cmp, i16 %val) nounwind {
; LA64-LABEL: cmpxchg_i16_monotonic_monotonic_reti1:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:    addi.d $r7, $zero, -4
; LA64-NEXT:    and $r7, $r4, $r7
; LA64-NEXT:    andi $r4, $r4, 3
; LA64-NEXT:    slli.w $r8, $r4, 3
; LA64-NEXT:    lu12i.w $r4, 15
; LA64-NEXT:    ori $r4, $r4, 4095
; LA64-NEXT:    sll.w $r9, $r4, $r8
; LA64-NEXT:    nor $r10, $zero, $r9
; LA64-NEXT:    and $r11, $r5, $r4
; LA64-NEXT:    sll.w $r11, $r11, $r8
; LA64-NEXT:    and $r4, $r6, $r4
; LA64-NEXT:    sll.w $r6, $r4, $r8
; LA64-NEXT:  .LBB29_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r12, $r7, 0
; LA64-NEXT:    and $r13, $r12, $r9
; LA64-NEXT:    bne $r13, $r11, .LBB29_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB29_1 Depth=1
; LA64-NEXT:    and $r12, $r12, $r10
; LA64-NEXT:    or $r12, $r12, $r6
; LA64-NEXT:    sc.w $r12, $r7, 0
; LA64-NEXT:    beq $r12, $zero, .LBB29_1
; LA64-NEXT:  .LBB29_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    srl.w $r4, $r13, $r8
; LA64-NEXT:    ext.w.h $r4, $r4
; LA64-NEXT:  # %bb.4:
; LA64-NEXT:    ext.w.h $r5, $r5
; LA64-NEXT:    xor $r4, $r4, $r5
; LA64-NEXT:    sltui $r4, $r4, 1
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i16* %ptr, i16 %cmp, i16 %val monotonic monotonic
  %res = extractvalue { i16, i1 } %tmp, 1
  ret i1 %res
}

define i1 @cmpxchg_i32_monotonic_monotonic_reti1(i32* %ptr, i32 %cmp, i32 %val) nounwind {
; LA64-LABEL: cmpxchg_i32_monotonic_monotonic_reti1:
; LA64:       # %bb.0:
; LA64-NEXT:    slli.w $r6, $r6, 0
; LA64-NEXT:    slli.w $r5, $r5, 0
; LA64-NEXT:  .LBB30_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.w $r7, $r4, 0
; LA64-NEXT:    bne $r7, $r5, .LBB30_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB30_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.w $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB30_1
; LA64-NEXT:  .LBB30_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    xor $r4, $r7, $r5
; LA64-NEXT:    sltui $r4, $r4, 1
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i32* %ptr, i32 %cmp, i32 %val monotonic monotonic
  %res = extractvalue { i32, i1 } %tmp, 1
  ret i1 %res
}

define i1 @cmpxchg_i64_monotonic_monotonic_reti1(i64* %ptr, i64 %cmp, i64 %val) nounwind {
; LA64-LABEL: cmpxchg_i64_monotonic_monotonic_reti1:
; LA64:       # %bb.0:
; LA64-NEXT:  .LBB31_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    ll.d $r7, $r4, 0
; LA64-NEXT:    bne $r7, $r5, .LBB31_3
; LA64-NEXT:  # %bb.2: # in Loop: Header=BB31_1 Depth=1
; LA64-NEXT:    move $r8, $r6
; LA64-NEXT:    sc.d $r8, $r4, 0
; LA64-NEXT:    beq $r8, $zero, .LBB31_1
; LA64-NEXT:  .LBB31_3:
; LA64-NEXT:    dbar 1792
; LA64-NEXT:    xor $r4, $r7, $r5
; LA64-NEXT:    sltui $r4, $r4, 1
; LA64-NEXT:    jr $ra
  %tmp = cmpxchg i64* %ptr, i64 %cmp, i64 %val monotonic monotonic
  %res = extractvalue { i64, i1 } %tmp, 1
  ret i1 %res
}
